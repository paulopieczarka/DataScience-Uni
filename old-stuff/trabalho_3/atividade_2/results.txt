PS D:\git-clones\DataScience-Uni\trabalho_3\atividade_2> py -u -W ignore .\kfold.py
identity 5 800 0.850241962863233
logistic 5 800 0.8463710236631548
tanh 5 800 0.8578882920080264
relu 5 800 0.8517338831545102
identity 6 800 0.8558298706908042
logistic 6 800 0.8526449228671293
tanh 6 800 0.859922783627583
relu 6 800 0.8511139552832059
identity 7 800 0.8550752994898844
logistic 7 800 0.8530516027499301
tanh 7 800 0.8546822641909018
relu 7 800 0.8586206655606585
identity 8 800 0.8566225850488557
logistic 8 800 0.8576983682576674
tanh 8 800 0.8529713344110741
relu 8 800 0.8605107235727611
identity 9 800 0.8527714955478902
logistic 9 800 0.8534344158184248
tanh 9 800 0.8582820432012715
relu 9 800 0.862662053476526
identity 10 800 0.8539986143462306
logistic 10 800 0.8608801170702138
tanh 10 800 0.8607497325275922
relu 10 800 0.8592925068401612
identity 5 900 0.8573230950691861
logistic 5 900 0.8585684318091096
tanh 5 900 0.8555619073613702
relu 5 900 0.8513159226732213
identity 6 900 0.8530389217242735
logistic 6 900 0.8580139753062559
tanh 6 900 0.8590600321249685
relu 6 900 0.8252772836027026
identity 7 900 0.8518352227069427
logistic 7 900 0.8585087683200567
tanh 7 900 0.8570906818615652
relu 7 900 0.8461059993460424
identity 8 900 0.8529481420376293
logistic 8 900 0.8602997570500804
tanh 8 900 0.8581721284530106
relu 8 900 0.8591332650211916
identity 9 900 0.853606404616639
logistic 9 900 0.857500243246724
tanh 9 900 0.8613688269011618
relu 9 900 0.8622021212790523
identity 10 900 0.8514375133914591
logistic 10 900 0.8590352266595213
tanh 10 900 0.8570742679824448
relu 10 900 0.8586709246872921
identity 5 1000 0.8569443287018732
logistic 5 1000 0.8570993112441572
tanh 5 1000 0.8542514220114843
relu 5 1000 0.8472091373758991
identity 6 1000 0.8485272628889691
logistic 6 1000 0.8595983237719776
tanh 6 1000 0.8546163601948094
relu 6 1000 0.8165906814612726
identity 7 1000 0.8535334788832198
logistic 7 1000 0.8598423930890728
tanh 7 1000 0.8575391504716177
relu 7 1000 0.8599409274704021
identity 8 1000 0.8532251342520827
logistic 8 1000 0.8571568418888621
tanh 8 1000 0.8606157861812503
relu 8 1000 0.859877385706603
identity 9 1000 0.8536043412026721
logistic 9 1000 0.8622292120981662
tanh 9 1000 0.8599814843897005
relu 9 1000 0.8573279539193779
identity 10 1000 0.8517989248331304
logistic 10 1000 0.8558722230302184
tanh 10 1000 0.8578484019541721
relu 10 1000 0.8561956671556281
identity 5 1100 0.8498535625000325
logistic 5 1100 0.8591305984377037
tanh 5 1100 0.8598465410359986
relu 5 1100 0.8584076355943898
identity 6 1100 0.8537113421579546
logistic 6 1100 0.8548148035819617
tanh 6 1100 0.8575710295698251
relu 6 1100 0.8608666029952893
identity 7 1100 0.8520846616502877
logistic 7 1100 0.8578648281558283
tanh 7 1100 0.8587585448303277
relu 7 1100 0.8588807163905441
identity 8 1100 0.8539889642132457
logistic 8 1100 0.8572004978619328
tanh 8 1100 0.8593255578497585
relu 8 1100 0.8607637823889241
identity 9 1100 0.8537478703006334
logistic 9 1100 0.8619832206866972
tanh 9 1100 0.8588785129624255
relu 9 1100 0.8558871646250529
identity 10 1100 0.8562010397630168
logistic 10 1100 0.8593344670358087
tanh 10 1100 0.8582209264746034
relu 10 1100 0.8603543394949744
identity 5 1200 0.8520754801259267
logistic 5 1200 0.8624818554287325
tanh 5 1200 0.8566820661828574
relu 5 1200 0.8400746564949475
identity 6 1200 0.8542695291480069
logistic 6 1200 0.8608155977773272
tanh 6 1200 0.8527753551826525
relu 6 1200 0.8437465964455333
identity 7 1200 0.8535156016805783
logistic 7 1200 0.8605602986655393
tanh 7 1200 0.8562689198407687
relu 7 1200 0.8602745348162781
identity 8 1200 0.8552468803893992
logistic 8 1200 0.8592467548485464
tanh 8 1200 0.8593005312707642
relu 8 1200 0.86023251071793
identity 9 1200 0.853018715773963
logistic 9 1200 0.8579006089297948
tanh 9 1200 0.8529100605523665
relu 9 1200 0.8493731620262752
identity 10 1200 0.8546139242516182
logistic 10 1200 0.86358846583709
tanh 10 1200 0.858978839099535
relu 10 1200 0.8582855496165823
logistic 10 1200 0.86358846583709
[[327  38  10   2   0   0   0]
 [  1 232  18   0   0   0   0]
 [  6  55 859  19   2   0   0]
 [  0   3   9 323  40   1   0]
 [  0   5   2  21 296  36  13]
 [  0   1   0   0  10 147  83]
 [  0   6   0   0   2  12 287]]
             precision    recall  f1-score   support

          1       0.98      0.87      0.92       377
          2       0.68      0.92      0.79       251
          3       0.96      0.91      0.93       941
          4       0.88      0.86      0.87       376
          5       0.85      0.79      0.82       373
          6       0.75      0.61      0.67       241
          7       0.75      0.93      0.83       307

avg / total       0.87      0.86      0.86      2866